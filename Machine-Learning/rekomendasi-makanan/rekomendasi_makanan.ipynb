{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyP3NrFstK4HIgjqP5ZYqlQD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import random\n","from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n","import numpy as np\n","import tensorflow as tf\n","\n","# Creating dummy dataset\n","food_names = [\n","    \"Nasi Goreng\", \"Sate Ayam\", \"Rendang\", \"Gudeg\", \"Pempek\",\n","    \"Soto Betawi\", \"Bakso\", \"Gado-Gado\", \"Ayam Penyet\", \"Es Cendol\",\n","    \"Kerak Telor\", \"Tahu Sumedang\", \"Ikan Bakar\", \"Karedok\", \"Nasi Uduk\",\n","    \"Mie Aceh\", \"Papeda\", \"Bubur Ayam\", \"Siomay Bandung\", \"Lumpia Semarang\",\n","    \"Soto Lamongan\", \"Ayam Rica-Rica\", \"Serabi Solo\", \"Lontong Balap\", \"Ketoprak\",\n","    \"Rawon\", \"Sop Buntut\", \"Bubur Kacang Hijau\", \"Es Doger\", \"Sop Kambing\",\n","    \"Krupuk\", \"Es Teler\", \"Opor Ayam\", \"Nasi Liwet\", \"Sambal Goreng\", \"Tahu Gejrot\",\n","    \"Tempe Mendoan\", \"Nasi Timbel\", \"Ayam Taliwang\", \"Sate Kambing\", \"Sate Lilit\",\n","    \"Sop Konro\", \"Kue Lumpur\", \"Pastel\", \"Martabak\", \"Gulai\", \"Bakmi Jawa\",\n","    \"Capcay\", \"Sambal Matah\", \"Ayam Betutu\", \"Sate Padang\", \"Kue Putu\", \"Es Podeng\",\n","    \"Gudangan\", \"Sayur Lodeh\", \"Empal Gentong\", \"Rawon Nguling\", \"Sate Maranggi\",\n","    \"Soto Banjar\", \"Rujak Cingur\", \"Tahu Petis\", \"Cilok\", \"Cimol\", \"Seblak\",\n","    \"Batagor\", \"Rendang Sapi\", \"Ikan Asin\", \"Bubur Sumsum\", \"Nasi Kuning\",\n","    \"Ikan Pindang\", \"Es Dawet\", \"Urap\", \"Ayam Kalasan\", \"Nasi Pecel\", \"Pecel Lele\",\n","    \"Lontong Sayur\", \"Rujak Manis\", \"Pepes Ikan\", \"Sate Taichan\", \"Sop Ikan\",\n","    \"Soto Kudus\", \"Tahu Bulat\", \"Lumpia Basah\", \"Kue Ape\", \"Kue Pukis\", \"Sop Ayam\",\n","    \"Sayur Asem\", \"Nasi Campur\", \"Es Campur\", \"Kue Pancong\", \"Kue Klepon\",\n","    \"Gulai Kambing\", \"Sate Kerang\", \"Sate Telur Puyuh\", \"Nasi Kapau\", \"Ayam Goreng\",\n","    \"Mie Goreng\", \"Sambal Ijo\", \"Sambal Merah\", \"Tahu Isi\", \"Bakwan Jagung\",\n","    \"Kolak Pisang\", \"Nasi Ulam\", \"Tumis Kangkung\", \"Ikan Bakar Rica\"\n","]\n","regions = [\"Jawa\", \"Sumatera\", \"Betawi\", \"Sulawesi\", \"Aceh\", \"Papua\", \"Bali\", \"Lombok\", \"Kalimantan\", \"Maluku\"]\n","tastes = [\"Gurih\", \"Pedas\", \"Manis\", \"Asam\", \"Segar\", \"Pahit\"]\n","\n","# Generating a dataset with 1000 entries\n","dummy_data = {\n","    \"food\": [random.choice(food_names) for _ in range(1000)],\n","    \"calories\": [random.randint(100, 1000) for _ in range(1000)],\n","    \"origin\": [random.choice(regions) for _ in range(1000)],\n","    \"taste\": [random.choice(tastes) for _ in range(1000)]\n","}\n","df_large = pd.DataFrame(dummy_data)\n","\n","# Preprocessing the data\n","label_encoder_origin = LabelEncoder()\n","label_encoder_taste = LabelEncoder()\n","df_large['origin_encoded'] = label_encoder_origin.fit_transform(df_large['origin'])\n","df_large['taste_encoded'] = label_encoder_taste.fit_transform(df_large['taste'])\n","df_large['category'] = df_large['taste']  # Grouping by taste for demonstration\n","label_encoder_category = LabelEncoder()\n","df_large['category_encoded'] = label_encoder_category.fit_transform(df_large['category'])\n","\n","# Feature normalization\n","scaler = MinMaxScaler()\n","df_large['calories_scaled'] = scaler.fit_transform(df_large[['calories']])\n","\n","# Preparing features and labels\n","X = df_large[['calories_scaled', 'origin_encoded', 'taste_encoded']].values\n","y = tf.keras.utils.to_categorical(df_large['category_encoded'], num_classes=len(df_large['category_encoded'].unique()))\n","\n","# Building and training the model\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Input(shape=(3,)),\n","    tf.keras.layers.Dense(64, activation='relu'),\n","    tf.keras.layers.Dropout(0.2),\n","    tf.keras.layers.Dense(32, activation='relu'),\n","    tf.keras.layers.Dense(len(df_large['category_encoded'].unique()), activation='softmax')\n","])\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","model.fit(X, y, epochs=10, batch_size=32, verbose=1)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E90a8tZBbRPR","executionInfo":{"status":"ok","timestamp":1731801842902,"user_tz":-420,"elapsed":6585,"user":{"displayName":"Muhammad Alvaro Khikman","userId":"09662903280046199337"}},"outputId":"304d37ac-32db-4727-9922-ee6b4bc1a0d5"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.2745 - loss: 1.7715\n","Epoch 2/10\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3733 - loss: 1.4806\n","Epoch 3/10\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4631 - loss: 1.3145\n","Epoch 4/10\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4621 - loss: 1.2289\n","Epoch 5/10\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5074 - loss: 1.1107 \n","Epoch 6/10\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5317 - loss: 1.0405 \n","Epoch 7/10\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5603 - loss: 0.9791 \n","Epoch 8/10\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5638 - loss: 0.9197\n","Epoch 9/10\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6187 - loss: 0.8603 \n","Epoch 10/10\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6241 - loss: 0.8355 \n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x7f00b4687100>"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# Save the model\n","model.save('food_recommendation_model.h5')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aUxIbt98bz-Y","executionInfo":{"status":"ok","timestamp":1731801949864,"user_tz":-420,"elapsed":1004,"user":{"displayName":"Muhammad Alvaro Khikman","userId":"09662903280046199337"}},"outputId":"28d2b657-aad2-4ce7-f6dd-bef1528804b9"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]}]},{"cell_type":"code","source":["# Load the model\n","loaded_model = tf.keras.models.load_model('food_recommendation_model.h5')\n","\n","# Example usage with the loaded model\n","def recommend_food_with_loaded_model(calories, origin, taste):\n","    origin_encoded = label_encoder_origin.transform([origin])[0]\n","    taste_encoded = label_encoder_taste.transform([taste])[0]\n","    calories_scaled = scaler.transform([[calories]])[0, 0]\n","    input_data = np.array([[calories_scaled, origin_encoded, taste_encoded]])\n","    predictions = loaded_model.predict(input_data)\n","    recommended_indices = predictions[0].argsort()[-3:][::-1]\n","    recommendations = df_large.iloc[recommended_indices]['food'].values\n","    return recommendations\n","\n","# Using the loaded model for prediction\n","user_calories = 350\n","user_origin = 'Jawa'\n","user_taste = 'Gurih'\n","recommendations = recommend_food_with_loaded_model(user_calories, user_origin, user_taste)\n","print(f\"Recommended foods for {user_calories} calories, origin {user_origin}, and taste {user_taste}: {recommendations}\")"],"metadata":{"id":"_yO6xJ-QcJ-A"},"execution_count":null,"outputs":[]}]}